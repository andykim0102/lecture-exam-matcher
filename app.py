import os
import re
import sqlite3
import joblib
import pandas as pd
import streamlit as st
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from streamlit_mic_recorder import mic_recorder

# =========================
# 1. ì´ˆê¸° ì„¤ì • ë° DB ì—°ê²°
# =========================
st.set_page_config(page_title="Med-Study AI Assistant", layout="wide")
APP_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (NameError ë°©ì§€)
if 'pre_analysis' not in st.session_state: st.session_state.pre_analysis = []
if 'bundle' not in st.session_state: st.session_state.bundle = None

def get_db_connection(user_id):
    conn = sqlite3.connect(os.path.join(DATA_DIR, f"{user_id}.db"))
    conn.execute("""
        CREATE TABLE IF NOT EXISTS exams (
            id INTEGER PRIMARY KEY, year TEXT, num TEXT, content TEXT, pattern TEXT
        )
    """)
    return conn

def get_pdf_text(file):
    reader = PdfReader(file)
    return [page.extract_text() or "" for page in reader.pages]

# =========================
# 2. ë©”ì¸ UI í™”ë©´
# =========================
st.title("ğŸ©º ìŠ¤ë§ˆíŠ¸ ê°•ì˜ë¡-ì¡±ë³´ ë§¤ì¹­ ë¹„ì„œ")
user_id = st.sidebar.text_input("ì‚¬ìš©ì ID", "demo_user")
conn = get_db_connection(user_id)

tab1, tab2, tab3 = st.tabs(["ğŸ“… ìˆ˜ì—… ì „: ìë™ ë¶„ì„", "ğŸ™ï¸ ìˆ˜ì—… ì¤‘: ì‹¤ì‹œê°„ ë§¤ì¹­", "ğŸ¯ ìˆ˜ì—… í›„: ë³µìŠµ"])

# --- [Tab 1: ìˆ˜ì—… ì „ ì‚¬ì „ ë¶„ì„] ---
with tab1:
    st.header("ê°•ì˜ì‹¤ ê°€ê¸° ì „: ê¸°ì¶œ í¬ì¸íŠ¸ ë¯¸ë¦¬ë³´ê¸°")
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("1. ì¡±ë³´ ë°ì´í„° ë“±ë¡")
        exam_files = st.file_uploader("ì¡±ë³´ PDF ë“±ë¡", type="pdf", accept_multiple_files=True)
        if st.button("ì¡±ë³´ ì¸ë±ì‹± ì‹œì‘"):
            for f in exam_files:
                texts = get_pdf_text(f)
                conn.executemany("INSERT INTO exams (year, num, content, pattern) VALUES (?,?,?,?)", 
                                 [("2024", "ë¯¸ì§€ì •", t, "ì¼ë°˜") for t in texts if t.strip()])
            conn.commit()
            
            # ì¸ë±ìŠ¤ êµ¬ì¶•
            rows = conn.execute("SELECT content FROM exams").fetchall()
            if rows:
                texts = [r[0] for r in rows]
                vec = TfidfVectorizer(ngram_range=(1, 2))
                mat = vec.fit_transform(texts)
                st.session_state.bundle = {"vectorizer": vec, "matrix": mat, "raw": rows}
                st.success("ì¡±ë³´ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")

    with col2:
        st.subheader("2. ì˜¤ëŠ˜ ê°•ì˜ë¡ ë¶„ì„")
        lec_file = st.file_uploader("ê°•ì˜ë¡ PDF", type="pdf")
        if lec_file and st.button("ìˆ˜ì—… ì „ ìë™ ë‹¨ê¶Œí™”"):
            if st.session_state.bundle:
                lec_pages = get_pdf_text(lec_file)
                bundle = st.session_state.bundle
                analysis = []
                for i, p_text in enumerate(lec_pages):
                    if not p_text.strip(): continue
                    qv = bundle["vectorizer"].transform([p_text])
                    sims = cosine_similarity(qv, bundle["matrix"]).flatten()
                    if sims.max() > 0.25:
                        best_idx = sims.argmax()
                        analysis.append({"page": i+1, "score": sims.max(), "content": bundle["raw"][best_idx][0]})
                st.session_state.pre_analysis = analysis
                st.success(f"ë¶„ì„ ì™„ë£Œ! {len(analysis)}ê°œ í˜ì´ì§€ì—ì„œ ì¡±ë³´ ê´€ë ¨ì„± ë°œê²¬.")
            else:
                st.error("ë¨¼ì € ì¡±ë³´ë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”.")

# --- [Tab 2: ìˆ˜ì—… ì¤‘ ì‹¤ì‹œê°„ ë§¤ì¹­] ---
with tab2:
    st.header("ğŸ§ ì‹¤ì‹œê°„ ê°•ì˜ íŠ¸ë˜í‚¹")
    if not st.session_state.pre_analysis:
        st.warning("ìˆ˜ì—… ì „ ë¶„ì„ì„ ë¨¼ì € ì™„ë£Œí•´ ì£¼ì„¸ìš”.")
    else:
        st.info("ë…¹ìŒ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ êµìˆ˜ë‹˜ì˜ ì„¤ëª…ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì¡±ë³´ ì •ë³´ë¥¼ ì¦‰ì‹œ ë„ì›ë‹ˆë‹¤.")
        
        # ì‹¤ì œ ìŒì„± ë…¹ìŒ ë„êµ¬
        audio = mic_recorder(start_prompt="ğŸ”´ êµìˆ˜ë‹˜ ì„¤ëª… ë…¹ìŒ ì‹œì‘", stop_prompt="â¹ï¸ ì¤‘ì§€ ë° ë¶„ì„", key='recorder')
        
        if audio:
            st.audio(audio['bytes'])
            # ë°ëª¨ìš©: ì¸ì‹ëœ í…ìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ ì‹œ OpenAI Whisper ë“± ì—°ë™)
            simulated_speech = "ì´ ìˆ˜ìš©ì²´ ê¸°ì „ì€ ì‘ë…„ êµ­ì‹œì—ë„ ë‚˜ì™”ê³  ì•„ì£¼ ì¤‘ìš”í•©ë‹ˆë‹¤."
            st.subheader(f"ì¸ì‹ëœ ê°•ì˜ ë‚´ìš©: {simulated_speech}")
            
            # ì‹¤ì‹œê°„ ë§¤ì¹­ ì•Œë¦¼
            hits = [item for item in st.session_state.pre_analysis if any(word in item['content'] for word in simulated_speech.split()[:3])]
            if hits:
                for hit in hits:
                    st.toast(f"ğŸ”¥ ì¡±ë³´ ì ì¤‘! ê°•ì˜ë¡ {hit['page']}p", icon="ğŸš¨")
                    with st.warning():
                        st.markdown(f"### ğŸš¨ ì‹¤ì‹œê°„ ì¡±ë³´ ì ì¤‘ (ê°•ì˜ë¡ {hit['page']}í˜ì´ì§€ ê´€ë ¨)")
                        st.write(f"**ê³¼ê±° ê¸°ì¶œ ë‚´ìš©:** {hit['content'][:200]}...")
            else:
                st.write("í˜„ì¬ ë°œì–¸ê³¼ ì¼ì¹˜í•˜ëŠ” ì¡±ë³´ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# --- [Tab 3: ìˆ˜ì—… í›„ ë³µìŠµ] ---
with tab3:
    st.header("ì˜¤ëŠ˜ì˜ ìš”ì•½ ë° ë‹¨ê¶Œí™”")
    if st.session_state.pre_analysis:
        df_results = pd.DataFrame(st.session_state.pre_analysis)
        st.dataframe(df_results)
        
        # Anki ì¹´ë“œ ìƒì„± ê¸°ëŠ¥
        csv = df_results.to_csv(index=False).encode('utf-8')
        st.download_button("ğŸ“¥ Anki ì¹´ë“œìš© CSV ë‹¤ìš´ë¡œë“œ", csv, "anki_cards.csv", "text/csv")
    else:
        st.write("í‘œì‹œí•  ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
