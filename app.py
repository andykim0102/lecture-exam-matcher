# app.py
import time
import re
import streamlit as st
import google.generativeai as genai
import fitz  # PyMuPDF
from PIL import Image
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# ==========================================
# 0. Page config
# ==========================================
st.set_page_config(page_title="Med-Study OS", layout="wide", page_icon="ğŸ©º")
st.caption("ğŸ“Œ íë¦„: (1) ê³¼ëª©ë³„ ì¡±ë³´ ì—…ë¡œë“œâ†’DB êµ¬ì¶•  (2) ê°•ì˜ë³¸/ì „ì‚¬í…ìŠ¤íŠ¸ â†’ ì¡°êµê°€ 'ì¡±ë³´ ë‚˜ì˜¨ í¬ì¸íŠ¸'ë§Œ ìš”ì•½")

# ==========================================
# 1. Session state
# ==========================================
if "db" not in st.session_state:
    # item: {"subject": str, "page": int, "text": str, "source": str, "embedding": list[float]}
    st.session_state.db = []

if "api_key" not in st.session_state:
    st.session_state.api_key = None

if "api_key_ok" not in st.session_state:
    st.session_state.api_key_ok = False

if "text_models" not in st.session_state:
    st.session_state.text_models = []

if "best_text_model" not in st.session_state:
    st.session_state.best_text_model = None

if "lecture_doc" not in st.session_state:
    st.session_state.lecture_doc = None

if "lecture_filename" not in st.session_state:
    st.session_state.lecture_filename = None

if "current_page" not in st.session_state:
    st.session_state.current_page = 0

# caches
if "last_page_sig" not in st.session_state:
    st.session_state.last_page_sig = None

if "last_ai_sig" not in st.session_state:
    st.session_state.last_ai_sig = None

if "last_ai_text" not in st.session_state:
    st.session_state.last_ai_text = ""

if "last_related" not in st.session_state:
    st.session_state.last_related = []

# ==========================================
# 2. Settings
# ==========================================
JOKBO_THRESHOLD = 0.72  # ì¶”ì²œ 0.70~0.75


def has_jokbo_evidence(related: list[dict]) -> bool:
    return bool(related) and related[0]["score"] >= JOKBO_THRESHOLD


# ==========================================
# 3. Utils
# ==========================================
def ensure_configured():
    if st.session_state.get("api_key"):
        genai.configure(api_key=st.session_state["api_key"])


def extract_text_from_pdf(uploaded_file):
    """PDF -> pages [{page, text, source}]"""
    data = uploaded_file.getvalue()  # âœ… UploadedFile read() ì´ìŠˆ ë°©ì§€
    doc = fitz.open(stream=data, filetype="pdf")
    pages = []
    for i, page in enumerate(doc):
        text = page.get_text() or ""
        if text.strip():
            pages.append({"page": i + 1, "text": text, "source": uploaded_file.name})
    return pages


def get_embedding(text: str):
    text = (text or "").strip()
    if not text:
        return []
    text = text[:12000]  # ì•ˆì •ì„± ì»·
    ensure_configured()

    try:
        return genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type="retrieval_document",
        )["embedding"]
    except Exception:
        try:
            return genai.embed_content(
                model="models/embedding-001",
                content=text,
                task_type="retrieval_document",
            )["embedding"]
        except Exception:
            return []


def filter_db_by_subject(subject: str, db: list[dict]):
    """subjectê°€ 'ì „ì²´'ë©´ ì „ì²´ ë°˜í™˜, ì•„ë‹ˆë©´ í•´ë‹¹ ê³¼ëª©ë§Œ"""
    if not db:
        return []
    subject = (subject or "").strip()
    if subject in ["ì „ì²´", "ALL", ""]:
        return db
    return [x for x in db if x.get("subject") == subject]


def find_relevant_jokbo(query_text: str, db: list[dict], top_k: int = 5):
    """ìœ ì‚¬ë„ ê²€ìƒ‰"""
    if not db:
        return []

    query_emb = get_embedding(query_text)
    if not query_emb:
        return []

    valid_items = [item for item in db if item.get("embedding")]
    if not valid_items:
        return []

    db_embs = [item["embedding"] for item in valid_items]
    sims = cosine_similarity([query_emb], db_embs)[0]
    top_idxs = np.argsort(sims)[::-1][:top_k]
    return [{"score": float(sims[i]), "content": valid_items[i]} for i in top_idxs]


# ==========================================
# 4. AI (ì¡°êµ ì„¤ëª…)
# ==========================================
@st.cache_data(show_spinner=False)
def list_text_models(api_key: str):
    genai.configure(api_key=api_key)
    models = genai.list_models()
    out = []
    for m in models:
        methods = getattr(m, "supported_generation_methods", []) or []
        if "generateContent" in methods:
            out.append(m.name)
    return out


def pick_best_text_model(model_names: list[str]):
    if not model_names:
        return None
    flash = [m for m in model_names if "flash" in m.lower()]
    return flash[0] if flash else model_names[0]


def generate_with_fallback(prompt: str, model_names: list[str]):
    ensure_configured()
    last_err = None
    for name in model_names:
        if not name:
            continue
        try:
            model = genai.GenerativeModel(name)
            res = model.generate_content(prompt)
            text = getattr(res, "text", None)
            if text:
                return text, name
            return str(res), name
        except Exception as e:
            last_err = e
    raise last_err


def build_ta_prompt(lecture_text: str, related: list[dict], subject: str):
    """
    âœ… UIì—ëŠ” 'ì¡°êµ ì„¤ëª…'ë§Œ ì¶œë ¥
    âœ… ê·¼ê±°(ì¡±ë³´ ë°œì·Œ)ëŠ” í”„ë¡¬í”„íŠ¸ì—ë§Œ ë„£ê³  í™”ë©´ì—ì„œëŠ” ìˆ¨ê¹€
    """
    ctx_lines = []
    for r in related[:3]:
        c = r["content"]
        src = c.get("source", "")
        pg = c.get("page", "?")
        txt = (c.get("text") or "")[:450]
        ctx_lines.append(f'- [{src} p{pg} | sim={r["score"]:.3f}] {txt}')

    jokbo_ctx = "\n".join(ctx_lines)

    return f"""
ë„ˆëŠ” ì˜ëŒ€ ì¡°êµë‹¤. í•™ìƒì´ ê°•ì˜ë¥¼ ë“£ëŠ” ì¤‘ì´ë©°, ì§€ê¸ˆ í…ìŠ¤íŠ¸ê°€ ì¡±ë³´ì—ì„œ ì–´ë–¤ ì‹ìœ¼ë¡œ ì¶œì œë˜ì—ˆëŠ”ì§€ ë¹ ë¥´ê²Œ ì¡ì•„ì¤˜ì•¼ í•œë‹¤.
ê³¼ëª©: {subject}

ì¤‘ìš” ê·œì¹™:
- ì•„ë˜ [ê´€ë ¨ ì¡±ë³´ ë°œì·Œ]ì— ê·¼ê±°í•´ì„œë§Œ ë§í•´ë¼. (ì¶”ì¸¡/ìƒì‹ ì„¤ëª…/ì°½ì‘ ê¸ˆì§€)
- í•™ìƒì´ "ìˆ˜ì—… ì¤‘" ë°”ë¡œ ì²´í¬í•  ìˆ˜ ìˆê²Œ ì§§ê³  ëª…í™•í•˜ê²Œ.
- ê°•ì˜ í…ìŠ¤íŠ¸ë¥¼ ê¸¸ê²Œ ë‹¤ì‹œ ë§í•˜ì§€ ë§ˆë¼. ì¶œì œ í¬ì¸íŠ¸ë§Œ.
- ê·¼ê±° ì¸ìš©ì€ 2ê°œ ì´ìƒ í¬í•¨í•˜ë˜, í™”ë©´ì—ëŠ” ì¡±ë³´ ì›ë¬¸ì„ ê¸¸ê²Œ ë¶™ì´ì§€ ë§ê³  "ì§§ì€ êµ¬ì ˆ"ë¡œë§Œ.

ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ì§€ì¼œë¼):
[ì¡°êµ í•œì¤„ ì½”ë©˜íŠ¸]
- (í•œ ë¬¸ì¥)

[ì¡±ë³´ì—ì„œ ë‚˜ì˜¨ í¬ì¸íŠ¸ TOP3]
- (ì§§ê²Œ 3ê°œ)

[ì¡±ë³´ ì¶œì œ ë°©ì‹]
- (ê°ê´€ì‹/ë‹¨ë‹µ/ì„œìˆ /ë¹„êµ/ì •ì˜/ê¸°ì „ ë“±) + í•œ ì¤„

[ê·¼ê±°(ì§§ì€ ì¸ìš© 2ê°œ ì´ìƒ)]
- "..." (ì¶œì²˜: íŒŒì¼ëª… pí˜ì´ì§€)
- "..." (ì¶œì²˜: íŒŒì¼ëª… pí˜ì´ì§€)

[í•™ìƒ ì•¡ì…˜]
- ì§€ê¸ˆ ì™¸ìš¸ í‚¤ì›Œë“œ 5ê°œ: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, ...

[ì…ë ¥ í…ìŠ¤íŠ¸]
{lecture_text}

[ê´€ë ¨ ì¡±ë³´ ë°œì·Œ]
{jokbo_ctx}
""".strip()


def build_transcript_prompt(chunks: list[str], related_packs: list[list[dict]], subject: str):
    """
    ì „ì‚¬ í…ìŠ¤íŠ¸(ì—¬ëŸ¬ chunk)ì—ì„œ 'ì¡±ë³´ ê´€ë ¨ ë‚´ìš©ë§Œ' ë½‘ì•„ì„œ ì¡°êµê°€ ì •ë¦¬
    """
    # ê·¼ê±°ëŠ” chunkë³„ top2 ì •ë„ë§Œ
    lines = []
    for idx, (chunk, rel) in enumerate(zip(chunks, related_packs), start=1):
        if not has_jokbo_evidence(rel):
            continue
        ctx = []
        for r in rel[:2]:
            c = r["content"]
            ctx.append(f'- [{c.get("source","")} p{c.get("page","?")} sim={r["score"]:.3f}] {(c.get("text","")[:250])}')
        lines.append(f"""
(êµ¬ê°„ {idx})
[ê°•ì˜ ì „ì‚¬ ì¼ë¶€]
{chunk}

[ê´€ë ¨ ì¡±ë³´ ë°œì·Œ]
{chr(10).join(ctx)}
""".strip())

    packed = "\n\n".join(lines)
    if not packed.strip():
        packed = "(ì¡±ë³´ ê·¼ê±°ê°€ ìˆëŠ” êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.)"

    return f"""
ë„ˆëŠ” ì˜ëŒ€ ì¡°êµë‹¤. ì•„ë˜ëŠ” ê°•ì˜ ì „ì‚¬ í…ìŠ¤íŠ¸ ì¼ë¶€ êµ¬ê°„ë“¤ì´ë‹¤.
ëª©í‘œ: 'ì¡±ë³´ì— ì‹¤ì œë¡œ ë‚˜ì™”ë˜ ë‚´ìš©'ì— í•´ë‹¹í•˜ëŠ” êµ¬ê°„ë§Œ ê³¨ë¼, í•™ìƒì´ ë³µìŠµ/ìˆ˜ì—… ì¤‘ í¬ì¸íŠ¸ë¥¼ ì¡ê²Œ ìš”ì•½í•´ë¼.
ê³¼ëª©: {subject}

ì¤‘ìš” ê·œì¹™:
- ê° êµ¬ê°„ì€ ë°˜ë“œì‹œ [ê´€ë ¨ ì¡±ë³´ ë°œì·Œ] ê·¼ê±°ê°€ ìˆì„ ë•Œë§Œ í¬í•¨í•´ë¼.
- ì¶”ì¸¡ ê¸ˆì§€. ì¡±ë³´ ë°œì·Œ ê¸°ë°˜ìœ¼ë¡œë§Œ.
- ê²°ê³¼ëŠ” "ì¡±ë³´ í¬ì¸íŠ¸ ë…¸íŠ¸" í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ.

ì¶œë ¥ í˜•ì‹:
[ì¡±ë³´ í¬ì¸íŠ¸ ë…¸íŠ¸]
1) (í¬ì¸íŠ¸ ì œëª©) - í•œ ì¤„ ì„¤ëª…
   - ê·¼ê±°: "..." (ì¶œì²˜)
   - í•™ìƒ ì•¡ì…˜ í‚¤ì›Œë“œ: ...

2) ...

ì…ë ¥:
{packed}
""".strip()


# ==========================================
# 5. Transcript chunking
# ==========================================
def chunk_transcript(text: str, max_chars: int = 900):
    """
    ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ë¶„í• .
    - ë¹ˆ ì¤„/ë¬¸ì¥ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê³ 
    - ê¸¸ë©´ max_chars ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ë¶„í• 
    """
    text = (text or "").strip()
    if not text:
        return []

    # 1ì°¨: ë¹ˆ ì¤„ ê¸°ì¤€
    parts = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]

    # 2ì°¨: ë„ˆë¬´ ê¸´ ë©ì–´ë¦¬ ë¶„í• 
    chunks = []
    for p in parts:
        if len(p) <= max_chars:
            chunks.append(p)
        else:
            start = 0
            while start < len(p):
                chunks.append(p[start:start + max_chars])
                start += max_chars
    return chunks


# ==========================================
# 6. Sidebar
# ==========================================
with st.sidebar:
    st.title("ğŸ©º Med-Study")

    api_key = st.text_input("Gemini API Key", type="password", key="api_key_input")
    if api_key:
        try:
            st.session_state.api_key = api_key
            genai.configure(api_key=api_key)
            available_models = list_text_models(api_key)
            if not available_models:
                st.session_state.api_key_ok = False
                st.error("generateContent ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.session_state.api_key_ok = True
                st.session_state.text_models = available_models
                st.session_state.best_text_model = pick_best_text_model(available_models)
                st.success("AI ì—°ê²° ì™„ë£Œ")
                st.caption(f"í…ìŠ¤íŠ¸ ëª¨ë¸(ìë™): {st.session_state.best_text_model}")
        except Exception as e:
            st.session_state.api_key_ok = False
            st.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    st.divider()

    # ê³¼ëª© ëª©ë¡(í˜„ì¬ DB ê¸°ë°˜)
    subjects_in_db = sorted({x.get("subject", "") for x in st.session_state.db if x.get("subject")})
    st.caption(f"ğŸ“š í•™ìŠµëœ ì¡±ë³´ í˜ì´ì§€ ìˆ˜: **{len(st.session_state.db)}**")
    st.caption(f"ğŸ“š í•™ìŠµëœ ê³¼ëª©: **{', '.join(subjects_in_db) if subjects_in_db else '(ì—†ìŒ)'}**")

    if st.button("ì¡±ë³´ DB ì´ˆê¸°í™”", key="reset_db_btn"):
        st.session_state.db = []
        st.session_state.last_page_sig = None
        st.session_state.last_ai_sig = None
        st.session_state.last_ai_text = ""
        st.session_state.last_related = []
        st.rerun()


# ==========================================
# 7. Tabs
# ==========================================
tab1, tab2, tab3 = st.tabs(
    ["ğŸ“‚ 1) ê³¼ëª©ë³„ ì¡±ë³´ ì—…ë¡œë“œ/í•™ìŠµ", "ğŸ“– 2) ê°•ì˜ë³¸(PDF) â†’ ì¡°êµ ì„¤ëª…", "ğŸ™ï¸ 3) ê°•ì˜ ì „ì‚¬ í…ìŠ¤íŠ¸ â†’ ì¡±ë³´ í¬ì¸íŠ¸"]
)

# ==================================================
# TAB 1 â€” Subject-separated Jokbo DB build
# ==================================================
with tab1:
    st.header("ğŸ“‚ 1) ê³¼ëª©ë³„ ì¡±ë³´ ì—…ë¡œë“œ/í•™ìŠµ")
    st.info("ì—…ë¡œë“œ ì‹œ ê³¼ëª©ì„ ì§€ì •í•˜ë©´, ì´í›„ ë¶„ì„ì€ í•´ë‹¹ ê³¼ëª© DBì—ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    c1, c2 = st.columns([1, 2])
    with c1:
        subject_for_upload = st.selectbox(
            "ê³¼ëª©(ì¡±ë³´ ì—…ë¡œë“œìš©)",
            ["í•´ë¶€í•™", "ìƒë¦¬í•™", "ì•½ë¦¬í•™", "ê¸°íƒ€(ì§ì ‘ì…ë ¥)"],
            index=1,
            key="subject_upload_select",
        )
    with c2:
        subject_custom = st.text_input(
            "ê¸°íƒ€ ê³¼ëª©ëª…",
            disabled=(subject_for_upload != "ê¸°íƒ€(ì§ì ‘ì…ë ¥)"),
            key="subject_upload_custom",
        )

    subject_final = subject_custom.strip() if subject_for_upload == "ê¸°íƒ€(ì§ì ‘ì…ë ¥)" else subject_for_upload
    subject_final = subject_final if subject_final else "ê¸°íƒ€(ë¯¸ì…ë ¥)"

    st.caption(f"í˜„ì¬ ì—…ë¡œë“œ ê³¼ëª©: **{subject_final}**")

    files = st.file_uploader(
        "ì¡±ë³´ PDF ì—…ë¡œë“œ(ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type="pdf",
        accept_multiple_files=True,
        key="jokbo_pdf_uploader",
    )

    col_a, col_b = st.columns([1, 2])
    with col_a:
        max_pages = st.number_input(
            "íŒŒì¼ë‹¹ ìµœëŒ€ í•™ìŠµ í˜ì´ì§€(ë°ëª¨ìš©)",
            min_value=1,
            max_value=500,
            value=60,
            step=1,
            key="max_pages_input",
        )
    with col_b:
        st.caption("ë§ì´ í•™ìŠµí• ìˆ˜ë¡ ë¹„ìš©/ì‹œê°„ì´ ëŠ˜ì–´ìš”. (ë°ëª¨ëŠ” 30~80 ì¶”ì²œ)")

    if st.button("ğŸ“š ì¡±ë³´ DB êµ¬ì¶• ì‹œì‘", key="build_db_btn"):
        if not st.session_state.api_key_ok or not st.session_state.get("api_key"):
            st.error("ì‚¬ì´ë“œë°”ì—ì„œ ìœ íš¨í•œ API Keyë¥¼ ë¨¼ì € ì„¤ì •í•˜ì„¸ìš”.")
            st.stop()
        if not files:
            st.warning("ì¡±ë³´ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()

        bar = st.progress(0)
        status = st.empty()
        new_db = []
        total_files = len(files)

        for i, f in enumerate(files):
            status.text(f"ğŸ“– íŒŒì¼ ì²˜ë¦¬: {f.name}")
            pages = extract_text_from_pdf(f)[: int(max_pages)]
            if not pages:
                status.text(f"âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ: {f.name} (ìŠ¤ìº”ë³¸ì´ë©´ OCR í•„ìš”)")
                bar.progress((i + 1) / total_files)
                continue

            for j, p in enumerate(pages):
                status.text(f"ğŸ§  ì„ë² ë”©: {f.name} ({j+1}/{len(pages)}p)")
                emb = get_embedding(p["text"])
                if emb:
                    p["embedding"] = emb
                    p["subject"] = subject_final
                    new_db.append(p)
                time.sleep(0.7)  # 429 ì™„í™”(êµ¬ì¶• ì‹œì—ë§Œ)

            bar.progress((i + 1) / total_files)

        st.session_state.db.extend(new_db)
        status.text("âœ… í•™ìŠµ ì™„ë£Œ")
        st.success(f"ê³¼ëª© [{subject_final}]ë¡œ ì´ {len(new_db)} í˜ì´ì§€ í•™ìŠµ ì™„ë£Œ")
        st.info("ğŸ‘‰ 2ë²ˆ/3ë²ˆ íƒ­ì—ì„œ ê³¼ëª©ì„ ì„ íƒí•˜ê³  ë¶„ì„í•˜ì„¸ìš”.")


# ==================================================
# TAB 2 â€” Lecture PDF -> TA explanation only (no raw jokbo UI)
# ==================================================
with tab2:
    st.header("ğŸ“– 2) ê°•ì˜ë³¸(PDF) â†’ ì¡°êµ ì„¤ëª…")
    st.info("ê°•ì˜ í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©´, ì˜¤ë¥¸ìª½ì— ì¡°êµê°€ 'ì¡±ë³´ì—ì„œ ë‚˜ì˜¨ í¬ì¸íŠ¸'ë§Œ ì„¤ëª…í•´ì¤ë‹ˆë‹¤. (ì›ë¬¸ ì¹´ë“œ ì¶œë ¥ ì—†ìŒ)")

    if not st.session_state.db:
        st.warning("ë¨¼ì € 1ë²ˆ íƒ­ì—ì„œ **ì¡±ë³´ DBë¥¼ êµ¬ì¶•**í•˜ì„¸ìš”.")

    # subject selection for analysis
    subjects_in_db = sorted({x.get("subject", "") for x in st.session_state.db if x.get("subject")})
    subject_options = ["ì „ì²´"] + (subjects_in_db if subjects_in_db else ["(DB ì—†ìŒ)"])
    subject_pick = st.selectbox("ë¶„ì„ ê³¼ëª©(ì´ ê³¼ëª© DBì—ì„œë§Œ ê²€ìƒ‰)", subject_options, key="tab2_subject_pick")

    lec_file = st.file_uploader("ê°•ì˜ë³¸ PDF ì—…ë¡œë“œ", type="pdf", key="lec_pdf_uploader")

    # (optional) debug: show evidence snippets
    debug_show = st.toggle("ë””ë²„ê·¸: ê·¼ê±°(ì§§ì€ ë°œì·Œ) ë³´ê¸°", value=False, key="debug_evidence_tab2")

    if lec_file:
        if st.session_state.lecture_doc is None or st.session_state.lecture_filename != lec_file.name:
            data = lec_file.getvalue()
            st.session_state.lecture_doc = fitz.open(stream=data, filetype="pdf")
            st.session_state.lecture_filename = lec_file.name
            st.session_state.current_page = 0
            st.session_state.last_page_sig = None
            st.session_state.last_ai_sig = None
            st.session_state.last_ai_text = ""
            st.session_state.last_related = []

        doc = st.session_state.lecture_doc
        col_view, col_right = st.columns([6, 4])

        # LEFT: PDF viewer
        with col_view:
            nav1, nav2, nav3 = st.columns([1, 2, 1])

            if nav1.button("â—€", key="prev_page_btn"):
                if st.session_state.current_page > 0:
                    st.session_state.current_page -= 1

            nav2.markdown(
                f"<center><b>{st.session_state.current_page+1} / {len(doc)}</b></center>",
                unsafe_allow_html=True,
            )

            if nav3.button("â–¶", key="next_page_btn"):
                if st.session_state.current_page < len(doc) - 1:
                    st.session_state.current_page += 1

            page = doc.load_page(st.session_state.current_page)
            pix = page.get_pixmap(dpi=150)
            st.image(
                Image.frombytes("RGB", [pix.width, pix.height], pix.samples),
                use_container_width=True,
            )

            page_text = (page.get_text() or "").strip()
            if not page_text:
                st.warning("ì´ í˜ì´ì§€ì—ëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. (ìŠ¤ìº” PDFë©´ OCRì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”)")

        # RIGHT: TA explain
        with col_right:
            st.markdown("### ğŸ§‘â€ğŸ« ì¡°êµ ì„¤ëª…")

            if not st.session_state.db:
                st.info("ì¡±ë³´ DBê°€ ì—†ì–´ì„œ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

            db_sub = filter_db_by_subject(subject_pick, st.session_state.db)

            if not page_text:
                st.info("í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

            # search only when page changes
            page_sig = hash(page_text)
            if page_sig != st.session_state.last_page_sig:
                st.session_state.last_page_sig = page_sig
                st.session_state.last_related = find_relevant_jokbo(page_text, db_sub, top_k=5)
                st.session_state.last_ai_sig = None  # force regen for new page

            related = st.session_state.last_related

            if not has_jokbo_evidence(related):
                st.warning("ì´ í˜ì´ì§€ëŠ” ì¡±ë³´ ê·¼ê±°ê°€ ëšœë ·í•˜ì§€ ì•Šì•„ì„œ(ì„ê³„ê°’ ë¯¸ë§Œ) ì¡°êµ ì„¤ëª…ì„ ìƒëµí–ˆìŠµë‹ˆë‹¤.")
                st.caption(f"ì„ê³„ê°’: {JOKBO_THRESHOLD:.2f} / ìµœê³  ìœ ì‚¬ë„: {related[0]['score']:.3f}" if related else "")
                st.stop()

            if not st.session_state.api_key_ok:
                st.warning("ì¡°êµ ì„¤ëª…ì„ ì“°ë ¤ë©´ ì‚¬ì´ë“œë°”ì— Gemini API Keyë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
                st.stop()

            # AI caching
            ai_sig = (page_sig, subject_pick, related[0]["content"].get("source"), related[0]["content"].get("page"))
            if ai_sig != st.session_state.last_ai_sig:
                prompt = build_ta_prompt(page_text, related, subject_pick)
                models = st.session_state.text_models or []
                with st.spinner("ì¡°êµê°€ ì¡±ë³´ ê·¼ê±°ë¡œ ì„¤ëª… ì¤‘..."):
                    result, used = generate_with_fallback(prompt, models)
                st.session_state.last_ai_sig = ai_sig
                st.session_state.last_ai_text = result
                st.caption(f"ì‚¬ìš© ëª¨ë¸: {used}")

            st.write(st.session_state.last_ai_text)

            if debug_show:
                with st.expander("ë””ë²„ê·¸: ë§¤ì¹­ ê·¼ê±°(ìƒìœ„ 3ê°œ, ì§§ê²Œ)", expanded=False):
                    for i, r in enumerate(related[:3], start=1):
                        c = r["content"]
                        st.markdown(f"**#{i} sim={r['score']:.3f} Â· {c.get('source','')} p{c.get('page','?')} Â· ê³¼ëª©={c.get('subject','')}**")
                        st.write((c.get("text") or "")[:500] + "â€¦")
    else:
        st.caption("ê°•ì˜ë³¸ PDFë¥¼ ì˜¬ë¦¬ë©´, ì˜¤ë¥¸ìª½ì— ì¡°êµ ì„¤ëª…ì´ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")


# ==================================================
# TAB 3 â€” Transcript text -> pick only jokbo-related points
# ==================================================
with tab3:
    st.header("ğŸ™ï¸ 3) ê°•ì˜ ì „ì‚¬ í…ìŠ¤íŠ¸ â†’ ì¡±ë³´ í¬ì¸íŠ¸")
    st.info("êµìˆ˜ë‹˜ ê°•ì˜ë¥¼ ë…¹ìŒí•œ ë’¤ ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ë„£ìœ¼ë©´, 'ì¡±ë³´ì— ë‚˜ì˜¨ ë‚´ìš©'ë§Œ ê³¨ë¼ ì¡°êµê°€ ì •ë¦¬í•©ë‹ˆë‹¤. (ì›ë¬¸ ì¹´ë“œ ì¶œë ¥ ì—†ìŒ)")

    if not st.session_state.db:
        st.warning("ë¨¼ì € 1ë²ˆ íƒ­ì—ì„œ **ì¡±ë³´ DBë¥¼ êµ¬ì¶•**í•˜ì„¸ìš”.")

    subjects_in_db = sorted({x.get("subject", "") for x in st.session_state.db if x.get("subject")})
    subject_options = ["ì „ì²´"] + (subjects_in_db if subjects_in_db else ["(DB ì—†ìŒ)"])
    subject_pick = st.selectbox("ë¶„ì„ ê³¼ëª©(ì´ ê³¼ëª© DBì—ì„œë§Œ ê²€ìƒ‰)", subject_options, key="tab3_subject_pick")

    up_txt = st.file_uploader("ì „ì‚¬ í…ìŠ¤íŠ¸(.txt) ì—…ë¡œë“œ(ì„ íƒ)", type=["txt"], key="transcript_txt_uploader")
    transcript_text = ""
    if up_txt is not None:
        try:
            transcript_text = up_txt.getvalue().decode("utf-8", errors="ignore")
        except Exception:
            transcript_text = ""

    transcript_text = st.text_area(
        "ì „ì‚¬ í…ìŠ¤íŠ¸ ë¶™ì—¬ë„£ê¸°(ì—…ë¡œë“œ ëŒ€ì‹  ê°€ëŠ¥)",
        value=transcript_text,
        height=240,
        key="transcript_text_area",
        placeholder="ì˜ˆ) ì˜¤ëŠ˜ì€ ì‹ ê²½ê³„ì˜ ... (ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê¸°)",
    )

    col1, col2 = st.columns([1, 1])
    with col1:
        max_chunks = st.number_input("ìµœëŒ€ êµ¬ê°„ ìˆ˜(ë°ëª¨)", min_value=1, max_value=40, value=12, step=1, key="max_chunks")
    with col2:
        st.caption("ì „ì‚¬ í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´ ë¹„ìš©/ì‹œê°„ì´ ëŠ˜ì–´ìš”. ë°ëª¨ëŠ” 8~15 ì¶”ì²œ")

    debug_show = st.toggle("ë””ë²„ê·¸: êµ¬ê°„ë³„ ë§¤ì¹­ ì ìˆ˜ ë³´ê¸°", value=False, key="debug_evidence_tab3")

    if st.button("ğŸ§  ì „ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ì¡±ë³´ í¬ì¸íŠ¸ ë½‘ê¸°", key="run_transcript_btn"):
        if not transcript_text.strip():
            st.error("ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥(ë˜ëŠ” txt ì—…ë¡œë“œ)í•˜ì„¸ìš”.")
            st.stop()
        if not st.session_state.api_key_ok:
            st.error("ì‚¬ì´ë“œë°”ì— Gemini API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            st.stop()

        db_sub = filter_db_by_subject(subject_pick, st.session_state.db)

        # chunking
        chunks = chunk_transcript(transcript_text, max_chars=900)[: int(max_chunks)]
        if not chunks:
            st.error("í…ìŠ¤íŠ¸ë¥¼ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.stop()

        # retrieve per chunk
        related_packs = []
        prog = st.progress(0)
        for i, ch in enumerate(chunks, start=1):
            rel = find_relevant_jokbo(ch, db_sub, top_k=3)
            related_packs.append(rel)
            prog.progress(i / len(chunks))

        # build + run AI summarizer (only evidence chunks)
        prompt = build_transcript_prompt(chunks, related_packs, subject_pick)
        models = st.session_state.text_models or []

        with st.spinner("ì¡±ë³´ ê·¼ê±°ê°€ ìˆëŠ” êµ¬ê°„ë§Œ ëª¨ì•„ ì¡°êµê°€ ì •ë¦¬ ì¤‘..."):
            result, used = generate_with_fallback(prompt, models)

        st.markdown("### ğŸ§‘â€ğŸ« ì¡±ë³´ í¬ì¸íŠ¸ ë…¸íŠ¸")
        st.caption(f"ì‚¬ìš© ëª¨ë¸: {used}")
        st.write(result)

        if debug_show:
            with st.expander("ë””ë²„ê·¸: êµ¬ê°„ë³„ ìµœê³  ìœ ì‚¬ë„", expanded=False):
                for idx, rel in enumerate(related_packs, start=1):
                    best = rel[0]["score"] if rel else 0.0
                    mark = "âœ…" if (rel and best >= JOKBO_THRESHOLD) else "â€”"
                    st.write(f"{mark} êµ¬ê°„ {idx}: best_sim={best:.3f}")
