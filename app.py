import os
import re
import sqlite3
import time
import joblib
import pandas as pd
import streamlit as st
from dataclasses import dataclass
from typing import List, Tuple, Any
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# =========================
# 1. Config & Directory
# =========================
APP_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

def safe_filename(s: str) -> str:
    return re.sub(r"[^a-zA-Z0-9ê°€-íž£._-]+", "_", s.strip())[:64] or "user"

def user_dir(user_id: str) -> str:
    d = os.path.join(DATA_DIR, safe_filename(user_id))
    os.makedirs(d, exist_ok=True)
    return d

# =========================
# 2. Database & Search Logic
# =========================
def db_connect(user_id: str):
    conn = sqlite3.connect(os.path.join(user_dir(user_id), "user.db"))
    conn.execute("CREATE TABLE IF NOT EXISTS pages (id INTEGER PRIMARY KEY, doc_name TEXT, page_num INTEGER, text TEXT)")
    return conn

def normalize(text: str) -> str:
    return re.sub(r"\s+", " ", text.replace("\x00", " ")).strip()

def extract_pdf_pages(pdf_bytes: bytes):
    reader = PdfReader(pdf_bytes)
    return [(i + 1, normalize(p.extract_text() or "")) for i, p in enumerate(reader.pages)]

@dataclass
class IndexBundle:
    vectorizer: TfidfVectorizer
    matrix: Any
    page_ids: List[int]

def build_index(conn):
    rows = conn.execute("SELECT id, text FROM pages").fetchall()
    if not rows: return None
    texts, pids = [r[1] for r in rows], [r[0] for r in rows]
    vec = TfidfVectorizer(analyzer="char_wb", ngram_range=(3, 6))
    return IndexBundle(vec, vec.fit_transform(texts), pids)

def search_exam(conn, bundle: IndexBundle, query: str):
    qv = bundle.vectorizer.transform([query])
    sims = cosine_similarity(qv, bundle.matrix).flatten()
    results = []
    for i in sims.argsort()[::-1][:5]:
        if sims[i] <= 0.1: continue
        row = conn.execute("SELECT doc_name, page_num, text FROM pages WHERE id=?", (bundle.page_ids[i],)).fetchone()
        results.append({"score": float(sims[i]), "doc": row[0], "page": row[1], "text": row[2]})
    return results

# =========================
# 3. Main UI Flow
# =========================
st.set_page_config(page_title="ì˜ëŒ€ìƒ í•™ìŠµ OS", layout="wide")
st.title("ðŸ©º Med-Study OS: ì¡±ë³´ ë§¤ì¹­ & ì•”ê¸° ë¹„ì„œ")

# ì‚¬ì´ë“œë°”: ìœ ì € ê´€ë¦¬
user_id = st.sidebar.text_input("ì‚¬ìš©ìž ID", "medical_student_01")
conn = db_connect(user_id)
index_path = os.path.join(user_dir(user_id), "index.joblib")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë°ì´í„° íë¦„ ìœ ì§€ì˜ í•µì‹¬)
if 'match_data' not in st.session_state:
    st.session_state.match_data = None

tab1, tab2, tab3 = st.tabs(["ðŸ“¤ ì¡±ë³´/ê°•ì˜ë¡ ë“±ë¡", "âš¡ ìˆ˜ì—… ì¤‘ (Live)", "ðŸŽ¯ ìˆ˜ì—… í›„ (ë³µìŠµ)"])

# --- Tab 1: ë°ì´í„° ë¹Œë“œì—… ---
with tab1:
    st.header("í•™ê¸° ì´ˆ: ì¡±ë³´ ë° ê°•ì˜ë¡ ì¸ë±ì‹±")
    files = st.file_uploader("PDF ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True)
    if st.button("íŒŒì¼ ë°ì´í„°ë² ì´ìŠ¤ ì €ìž¥"):
        for f in files:
            pages = extract_pdf_pages(f.getvalue())
            conn.executemany("INSERT INTO pages (doc_name, page_num, text) VALUES (?, ?, ?)", 
                             [(f.name, p, t) for p, t in pages if t])
        conn.commit()
        st.success("ë°ì´í„° ì €ìž¥ ì™„ë£Œ!")

    if st.button("AI ê²€ìƒ‰ ì—”ì§„ ìµœì í™” (Index Build)"):
        bundle = build_index(conn)
        joblib.dump(bundle, index_path)
        st.success("ì¸ë±ì‹± ì™„ë£Œ! ì´ì œ ì‹¤ì‹œê°„ ë§¤ì¹­ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# --- Tab 2: ìˆ˜ì—… ì¤‘ ì‹¤ì‹œê°„ ì–´ì‹œìŠ¤í„´íŠ¸ ---
with tab2:
    st.header("ì‹¤ì‹œê°„ ê°•ì˜ ë§¤ì¹­ ì—”ì§„")
    bundle = joblib.load(index_path) if os.path.exists(index_path) else None
    
    if not bundle:
        st.warning("ë¨¼ì € Tab 1ì—ì„œ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
    else:
        live_note = st.text_area("âœï¸ êµìˆ˜ë‹˜ ê°•ì¡° ì‚¬í•­ / ì‹¤ì‹œê°„ í•„ê¸°", placeholder="êµìˆ˜ë‹˜ì´ ì–¸ê¸‰í•˜ì‹  í‚¤ì›Œë“œë¥¼ ì ìœ¼ì„¸ìš”...")
        
        if live_note:
            results = search_exam(conn, bundle, live_note)
            if results:
                st.session_state.match_data = results # ë³µìŠµ íƒ­ìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
                st.subheader("ðŸš¨ ê´€ë ¨ ê¸°ì¶œ ì¡±ë³´ íƒì§€!")
                for r in results:
                    with st.expander(f"ðŸ“ {r['doc']} (p.{r['page']}) - ìœ ì‚¬ë„ {int(r['score']*100)}%"):
                        st.write(r['text'])
                        st.progress(r['score'])
            else:
                st.info("í˜„ìž¬ ìž…ë ¥ê³¼ ê´€ë ¨ëœ ê³¼ê±° ê¸°ì¶œì´ ì—†ìŠµë‹ˆë‹¤.")

# --- Tab 3: ìˆ˜ì—… í›„ ì¸í…”ë¦¬ì „íŠ¸ ë³µìŠµ ---
with tab3:
    st.header("ë³µìŠµ ë° ì•”ê¸° ìµœì í™”")
    
    if not st.session_state.match_data:
        st.info("ìˆ˜ì—… ì¤‘ ë§¤ì¹­ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í•„ê¸°ë¥¼ ë¨¼ì € ì§„í–‰í•´ì£¼ì„¸ìš”.")
    else:
        df = pd.DataFrame(st.session_state.match_data)
        
        sub_tab1, sub_tab2, sub_tab3 = st.tabs(["ðŸ“ ë‹¨ê¶Œí™” ë¦¬í¬íŠ¸", "ðŸ§  ì•”ê¸°(Anki)", "ðŸ¤– AI ê¸°ì–µë²•"])
        
        with sub_tab1:
            st.subheader("ì˜¤ëŠ˜ì˜ ê¸°ì¶œ ìš°ì„ ìˆœìœ„")
            st.error(f"ê°€ìž¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ: {df.iloc[0]['doc']}ì˜ ê°œë…")
            st.table(df[['doc', 'page', 'score']])
            
        with sub_tab2:
            st.subheader("Anki ì¹´ë“œ ì¶”ì¶œ")
            anki_df = df[['doc', 'text']].rename(columns={'doc': 'Front', 'text': 'Back'})
            st.download_button("Ankiìš© CSV ë°›ê¸°", anki_df.to_csv(index=False).encode('utf-8'), "anki.csv")
            
        with sub_tab3:
            st.subheader("AI Mnemonics (ê¸°ì–µë²•)")
            topic = st.selectbox("ì•”ê¸°ê°€ í•„ìš”í•œ êµ¬ê°„", df['text'].str[:50])
            if st.button("ê¸°ì–µì˜ ê¶ì „ ìŠ¤í† ë¦¬ ìƒì„±"):
                st.success("ìƒì„± ì™„ë£Œ!")
                st.write(f"ðŸ‘‰ '{topic}...' ì„(ë¥¼) ì™¸ìš°ê¸° ìœ„í•´ ë‹¹ì‹ ì˜ ì±…ìƒ ìœ„ ì˜¤ë¥¸ìª½ ëª¨ì„œë¦¬ì— ì´ ê°œë…ì´ ë†“ì—¬ìžˆë‹¤ê³  ìƒìƒí•˜ì„¸ìš”!")
