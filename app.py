import os
import re
import sqlite3
from dataclasses import dataclass
from typing import List, Tuple, Any

import streamlit as st
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib

# =========================
# Config
# =========================
APP_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

def safe_filename(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[^a-zA-Z0-9ê°€-í£._-]+", "_", s)
    return s[:64] if s else "user"

def user_dir(user_id: str) -> str:
    d = os.path.join(DATA_DIR, safe_filename(user_id))
    os.makedirs(d, exist_ok=True)
    return d

def user_db_path(user_id: str) -> str:
    return os.path.join(user_dir(user_id), "user.db")

def user_index_path(user_id: str) -> str:
    return os.path.join(user_dir(user_id), "tfidf_index.joblib")

# =========================
# DB
# =========================
def db_connect(db_path: str):
    conn = sqlite3.connect(db_path)
    conn.execute("""
    CREATE TABLE IF NOT EXISTS pages (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        doc_name TEXT,
        page_num INTEGER,
        text TEXT
    )
    """)
    conn.commit()
    return conn

def db_insert_pages(conn, doc_name: str, pages: List[Tuple[int, str]]):
    conn.executemany(
        "INSERT INTO pages (doc_name, page_num, text) VALUES (?, ?, ?)",
        [(doc_name, p, t) for p, t in pages if t.strip()]
    )
    conn.commit()

def db_fetch_all(conn):
    cur = conn.execute("SELECT id, doc_name, page_num, text FROM pages")
    return cur.fetchall()

# =========================
# PDF
# =========================
def extract_pdf_pages(pdf_bytes: bytes):
    reader = PdfReader(pdf_bytes)
    pages = []
    for i, page in enumerate(reader.pages):
        try:
            text = page.extract_text() or ""
        except Exception:
            text = ""
        pages.append((i + 1, normalize(text)))
    return pages

def normalize(text: str) -> str:
    text = text.replace("\x00", " ")
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# =========================
# Index
# =========================
@dataclass
class IndexBundle:
    vectorizer: TfidfVectorizer
    matrix: Any
    page_ids: List[int]

def build_index(conn):
    rows = db_fetch_all(conn)
    texts = [r[3] for r in rows]
    page_ids = [r[0] for r in rows]

    vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(3, 6),
        min_df=1,
        max_df=0.95
    )
    matrix = vectorizer.fit_transform(texts)
    return IndexBundle(vectorizer, matrix, page_ids)

def search(conn, bundle: IndexBundle, query: str, k: int = 5):
    qv = bundle.vectorizer.transform([query])
    sims = cosine_similarity(qv, bundle.matrix).flatten()
    idxs = sims.argsort()[::-1][:k]

    results = []
    for i in idxs:
        if sims[i] <= 0:
            continue
        pid = bundle.page_ids[i]
        row = conn.execute(
            "SELECT doc_name, page_num, text FROM pages WHERE id=?",
            (pid,)
        ).fetchone()
        results.append({
            "score": float(sims[i]),
            "doc": row[0],
            "page": row[1],
            "text": row[2][:300] + "..."
        })
    return results

# =========================
# UI
# =========================
st.set_page_config(page_title="Lectureâ€“Exam Matcher", layout="wide")
st.title("ğŸ“š Lectureâ€“Exam Matcher (Demo)")

user_id = st.sidebar.text_input("User ID", "demo_user")
conn = db_connect(user_db_path(user_id))

index_path = user_index_path(user_id)
bundle = joblib.load(index_path) if os.path.exists(index_path) else None

tab1, tab2 = st.tabs(["ğŸ“¤ Upload & Index", "ğŸ” Search"])

with tab1:
    files = st.file_uploader("Upload PDFs", type="pdf", accept_multiple_files=True)
    if st.button("Save PDFs"):
        for f in files:
            pages = extract_pdf_pages(f.getvalue())
            db_insert_pages(conn, f.name, pages)
        st.success("PDFs saved")

    if st.button("Build Index"):
        bundle = build_index(conn)
        joblib.dump(bundle, index_path)
        st.success("Index built")

with tab2:
    query = st.text_input("Search keyword")
    if st.button("Search") and bundle:
        results = search(conn, bundle, query)
        for r in results:
            st.markdown(
                f"**{r['doc']} p.{r['page']}** (score {r['score']:.2f})"
            )
            st.write(r["text"])
# --- ê¸°ì¡´ ë§¤ì¹­ ê²°ê³¼ê°€ 'df_results'ë¼ëŠ” ë°ì´í„°í”„ë ˆì„ì— ìˆë‹¤ê³  ê°€ì •í•  ë•Œ ---

st.divider() # ì‹œê°ì  êµ¬ë¶„ì„ 
st.header("ğŸ¯ ìˆ˜ì—… í›„: ë³µìŠµ ë° ë‹¨ê¶Œí™” ì§€ì›")

# 1. íƒ­ìœ¼ë¡œ ê¸°ëŠ¥ ë¶„ë¥˜ (ê¹”ë”í•œ UI)
tab1, tab2, tab3 = st.tabs(["ğŸ“„ ë‹¨ê¶Œí™” ë…¸íŠ¸", "ğŸ§  ì•”ê¸° ì¹´ë“œ(Anki)", "ğŸ¤– AI í€´ì¦ˆ"])

with tab1:
    st.subheader("ì¡±ë³´ ì£¼ì„ í¬í•¨ PDF ìƒì„±")
    st.write("ê°•ì˜ë¡ì˜ ê¸°ì¶œ êµ¬ê°„ì— ì¡±ë³´ ë²ˆí˜¸ë¥¼ ìë™ìœ¼ë¡œ ì…íŒ PDFë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    if st.button("ë‹¨ê¶Œí™” PDF ë‹¤ìš´ë¡œë“œ (ì¤€ë¹„ ì¤‘)"):
        # ì—¬ê¸°ì— PyMuPDF(fitz) ë“±ì„ í™œìš©í•œ PDF í¸ì§‘ ë¡œì§ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.
        st.info("í˜„ì¬ ê°œë°œ ì¤‘ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ê¸°ì¶œ ìœ„ì¹˜ê°€ í‘œì‹œëœ ë ˆì´ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

with tab2:
    st.subheader("Anki ì¹´ë“œ ì„¸íŠ¸ ì¶”ì¶œ")
    st.write("ì˜¤ëŠ˜ ë§¤ì¹­ëœ ì¡±ë³´ ë¬¸í•­ì„ ê¸°ë°˜ìœ¼ë¡œ Anki(.csv) íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.")
    
    # ì˜ˆì‹œ ë°ì´í„° ìƒì„± ë¡œì§
    if not df_results.empty:
        anki_data = df_results[['lecture_keyword', 'exam_content']].rename(
            columns={'lecture_keyword': 'Front', 'exam_content': 'Back'}
        )
        csv = anki_data.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="Ankiìš© CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name='medical_anki_cards.csv',
            mime='text/csv',
        )
    else:
        st.warning("ë§¤ì¹­ëœ ë°ì´í„°ê°€ ì—†ì–´ ì¹´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.subheader("AI ì˜ˆì¸¡ ë³€í˜• ë¬¸ì œ")
    # GPT APIê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´ ë§¤ì¹­ëœ ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì „ë‹¬
    if st.button("ì˜¤ëŠ˜ì˜ í•µì‹¬ í€´ì¦ˆ ìƒì„±"):
        with st.spinner('AIê°€ ì¡±ë³´ íŒ¨í„´ì„ ë¶„ì„ ì¤‘...'):
            # ê°€ìƒì˜ ê²°ê³¼ ì˜ˆì‹œ
            st.success("ë¶„ì„ ì™„ë£Œ!")
            st.markdown("""
            **Q. ë‹¤ìŒ ì¤‘ ì˜¤ëŠ˜ ë°°ìš´ 'A ê¸°ì „'ì˜ ì¡±ë³´ ë¹ˆì¶œ ì˜¤ë‹µ ìœ í˜•ì€?**
            1. ì¦ìƒê³¼ ì•½ë¬¼ì„ ë°˜ëŒ€ë¡œ ë§¤ì¹­
            2. ë°œë³‘ ì‹œê¸°ë¥¼ 2ì£¼ì—ì„œ 4ì£¼ë¡œ ë³€ê²½
            3. ìœ ì „ í˜•ì‹ì„ ìš°ì„±ì—ì„œ ì—´ì„±ìœ¼ë¡œ ë³€ê²½
            
            *ì •ë‹µì€ **3ë²ˆ**ì…ë‹ˆë‹¤. ì‘ë…„ ì¡±ë³´ì—ì„œ ì´ ë¶€ë¶„ì´ í•¨ì •ìœ¼ë¡œ ë‚˜ì™”ìŠµë‹ˆë‹¤.*
            """)
import streamlit as st
import pandas as pd
# PDF ìƒì„± ë° ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìš” ì‹œ ì„¤ì¹˜: pip install fpdf)
from fpdf import FPDF 

# --- [ì°¨ë³„ì  1] ì¡±ë³´ ë§¤ì¹­ ê¸°ë°˜ ìš°ì„ ìˆœìœ„ ë° ì¸ì‚¬ì´íŠ¸ ---
st.divider()
st.header("ğŸš€ ì˜ëŒ€ìƒ ë§ì¶¤í˜• Post-Class ì—”ì§„")

# ê°€ìƒì˜ ë§¤ì¹­ ë°ì´í„°(df_results)ê°€ ìˆë‹¤ê³  ê°€ì •
if not df_results.empty:
    # ê¸°ì¶œ íšŸìˆ˜ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ê³„ì‚° ë¡œì§ ì¶”ê°€
    df_results['priority_score'] = df_results['match_count'] * 10  # ì˜ˆì‹œ ë¡œì§
    
    st.subheader("ğŸ“ ì˜¤ëŠ˜ ê°•ì˜ì˜ í•µì‹¬ 'ì¡±ë³´' í¬ì¸íŠ¸")
    top_picks = df_results.nlargest(3, 'priority_score')
    for i, row in top_picks.iterrows():
        st.error(f"**ì¤‘ìš”!** '{row['lecture_keyword']}' ê´€ë ¨ ë‚´ìš©ì€ ìµœê·¼ 5ë…„ê°„ {row['match_count']}íšŒ ì¶œì œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- [ì°¨ë³„ì  2] ë“œë˜ê·¸ & ë“œë¡­ ëŒ€ìš©: AI ë…¸íŠ¸ ì •ë¦¬ (ì•„ì´ë””ì–´ 3ë²ˆ ë°˜ì˜) ---
    st.subheader("ğŸ“ AI ìŠ¤ë§ˆíŠ¸ ë…¸íŠ¸ ìƒì„±")
    with st.expander("ê°•ì˜ë¡ê³¼ ì¡±ë³´ë¥¼ í•©ì¹œ 'ë‹¨ê¶Œí™” ì´ˆì•ˆ' ë³´ê¸°"):
        st.write("AIê°€ ë§¤ì¹­ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì•½ ë…¸íŠ¸ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        summary_text = ""
        for i, row in df_results.iterrows():
            summary_text += f"- **{row['lecture_keyword']}**: {row['exam_content']} (ê¸°ì¶œ: {row['year']}ë…„)\n"
        st.info(summary_text)
        
        # ë…¸íŠ¸ ì €ì¥ ê¸°ëŠ¥
        st.download_button("ë‚˜ë§Œì˜ ìš”ì•½ ë…¸íŠ¸(.txt) ì €ì¥", summary_text)

    # --- [ì°¨ë³„ì  3] ê¸°ì–µë²• ì„œë¹„ìŠ¤: ì•”ê¸° ìŠ¤í† ë¦¬í…”ë§ (ì•„ì´ë””ì–´ 4ë²ˆ ë°˜ì˜) ---
    st.subheader("ğŸ§  ì•”ê¸° ìµœì í™”: ê¸°ì–µì˜ ê¶ì „ & Mnemonics")
    selected_topic = st.selectbox("ì–´ë–¤ ê°œë…ì´ ì•ˆ ì™¸ì›Œì§€ë‚˜ìš”?", df_results['lecture_keyword'].unique())
    
    if st.button(f"'{selected_topic}' ì•”ê¸°ë²• ìƒì„±"):
        with st.spinner('ì•”ê¸° ìŠ¤í† ë¦¬ë¥¼ ë§Œë“œëŠ” ì¤‘...'):
            # ì‹¤ì œ ì„œë¹„ìŠ¤ ì‹œ GPT API ì—°ë™ êµ¬ê°„
            st.success("ìƒì„± ì™„ë£Œ! ì•„ë˜ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ì™¸ì›Œë³´ì„¸ìš”.")
            st.markdown(f"""
            > **Mnemonic Scenario:** > "{selected_topic}"ì„ ì™¸ìš°ê¸° ìœ„í•´ **[ê¸°ì–µì˜ ê¶ì „]** ê±°ì‹¤ì— ìˆëŠ” ì†ŒíŒŒë¥¼ ë– ì˜¬ë ¤ë³´ì„¸ìš”. 
            > ì†ŒíŒŒ ìœ„ì— {df_results[df_results['lecture_keyword']==selected_topic]['exam_content'].values[0]}ê°€ 
            > ê±°ëŒ€í•˜ê²Œ ë†“ì—¬ìˆë‹¤ê³  ìƒìƒí•˜ë©° ì—°ê²°í•˜ëŠ” ê²ë‹ˆë‹¤!
            """)

    # --- [ì°¨ë³„ì  4] Anki ì—°ë™ (ì‹¤í–‰ ì„±ëŠ¥ í–¥ìƒ) ---
    st.subheader("ğŸ“¥ ì™¸ë¶€ ì•± ì—°ë™")
    col1, col2 = st.columns(2)
    with col1:
        # CSV í¬ë§·ìœ¼ë¡œ Anki ì¹´ë“œ ìƒì„±
        anki_csv = df_results[['lecture_keyword', 'exam_content']].to_csv(index=False).encode('utf-8')
        st.download_button("Anki ì¹´ë“œ ì„¸íŠ¸(.csv) ë‹¤ìš´ë¡œë“œ", anki_csv, "anki_cards.csv", "text/csv")
    with col2:
        if st.button("iPad êµ¿ë…¸íŠ¸ìš© PDF ë‚´ë³´ë‚´ê¸°"):
            st.write("ë§¤ì¹­ ì£¼ì„ì´ í¬í•¨ëœ PDFë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")

else:
    st.warning("ë¨¼ì € ê°•ì˜ë¡ê³¼ ì¡±ë³´ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë§¤ì¹­ì„ ì§„í–‰í•´ ì£¼ì„¸ìš”.")

import streamlit as st
import time

# --- [ì°¨ë³„ì : ìˆ˜ì—… ì¤‘ ì‹¤ì‹œê°„ ì–´ì‹œìŠ¤í„´íŠ¸] ---
st.divider()
st.header("âš¡ ì‹¤ì‹œê°„ ìˆ˜ì—… ëª¨ë“œ (In-class Live)")

# ìˆ˜ì—… ì¤‘ ëª¨ë“œ í™œì„±í™” ìŠ¤ìœ„ì¹˜
live_mode = st.toggle("ì‹¤ì‹œê°„ ìˆ˜ì—… ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œì‘")

if live_mode:
    st.info("ğŸ¤ êµìˆ˜ë‹˜ì˜ ì„¤ëª…ì„ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì¡±ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.")
    
    # ë ˆì´ì•„ì›ƒ ë¶„í• : ì™¼ìª½(ì‹¤ì‹œê°„ í•„ê¸°/STT), ì˜¤ë¥¸ìª½(ì‹¤ì‹œê°„ ì¡±ë³´ ì•Œë¦¼)
    col_live, col_match = st.columns([1, 1])
    
    with col_live:
        st.subheader("ğŸ“ ì‹¤ì‹œê°„ ê°•ì˜ ìš”ì•½")
        # ì‹¤ì œ êµ¬í˜„ ì‹œ ìŒì„± ì¸ì‹(STT) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—°ë™ êµ¬ê°„
        user_note = st.text_area("êµìˆ˜ë‹˜ ê°•ì¡° ì‚¬í•­ì´ë‚˜ í•„ê¸°ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” ìŒì„± ì¸ì‹ ì¤‘...)", 
                                 placeholder="ì˜ˆ: 'ì´ ìˆ˜ìš©ì²´ ê¸°ì „ì€ ì‘ë…„ êµ­ì‹œì—ë„ ë‚˜ì™”ê³ ...'")
        
    with col_match:
        st.subheader("ğŸš¨ ì‹¤ì‹œê°„ ì¡±ë³´ ë§¤ì¹­")
        if user_note:
            # ì‹¤ì‹œê°„ ì…ë ¥ ë‚´ìš©ê³¼ ê¸°ì¡´ ì—…ë¡œë“œëœ df_results(ì¡±ë³´) ë§¤ì¹­ ì‹œë®¬ë ˆì´ì…˜
            with st.spinner('ê´€ë ¨ ê¸°ì¶œ í™•ì¸ ì¤‘...'):
                time.sleep(0.5) # ë¶„ì„ ì²˜ë¦¬ ì†ë„ ì‹œë®¬ë ˆì´ì…˜
                
                # ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ì¡±ë³´ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ ê°„ë‹¨ ì²´í¬
                matched_found = False
                for i, row in df_results.iterrows():
                    if row['lecture_keyword'] in user_note:
                        st.warning(f"**ê¸°ì¶œ ì¼ì¹˜!** [{row['year']}ë…„] {row['exam_content']}")
                        st.caption(f"ìš°ì„ ìˆœìœ„: {'ğŸ”¥'* (int(row['match_count']))}")
                        matched_found = True
                
                if not matched_found:
                    st.write("ì•„ì§ ì¼ì¹˜í•˜ëŠ” ê³¼ê±° ê¸°ì¶œ ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- [ì°¨ë³„ì : ì‹¤ì‹œê°„ ë“œë˜ê·¸ & ë“œë¡­ ëŒ€ì•ˆ] ---
    st.subheader("ğŸ“¸ ì‹¤ì‹œê°„ í™”ë©´ ìº¡ì²˜ ë° íƒœê¹…")
    if st.button("í˜„ì¬ ìŠ¬ë¼ì´ë“œ ì¡±ë³´ íƒœê·¸ì™€ í•¨ê»˜ ì €ì¥"):
        st.success("í˜„ì¬ ê°•ì˜ë¡ í˜ì´ì§€ê°€ 2023ë…„ ê¸°ì¶œ ì •ë³´ì™€ ë§¤ì¹­ë˜ì–´ 'ë‹¨ê¶Œí™” í›„ë³´'ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")

else:
    st.write("ìˆ˜ì—… ì‹œì‘ ì‹œ ìœ„ í† ê¸€ì„ ì¼œì£¼ì„¸ìš”.")
