import os
import re
import sqlite3
from dataclasses import dataclass
from typing import List, Tuple, Any

import streamlit as st
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib

# =========================
# Config
# =========================
APP_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

def safe_filename(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[^a-zA-Z0-9ê°€-íž£._-]+", "_", s)
    return s[:64] if s else "user"

def user_dir(user_id: str) -> str:
    d = os.path.join(DATA_DIR, safe_filename(user_id))
    os.makedirs(d, exist_ok=True)
    return d

def user_db_path(user_id: str) -> str:
    return os.path.join(user_dir(user_id), "user.db")

def user_index_path(user_id: str) -> str:
    return os.path.join(user_dir(user_id), "tfidf_index.joblib")

# =========================
# DB
# =========================
def db_connect(db_path: str):
    conn = sqlite3.connect(db_path)
    conn.execute("""
    CREATE TABLE IF NOT EXISTS pages (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        doc_name TEXT,
        page_num INTEGER,
        text TEXT
    )
    """)
    conn.commit()
    return conn

def db_insert_pages(conn, doc_name: str, pages: List[Tuple[int, str]]):
    conn.executemany(
        "INSERT INTO pages (doc_name, page_num, text) VALUES (?, ?, ?)",
        [(doc_name, p, t) for p, t in pages if t.strip()]
    )
    conn.commit()

def db_fetch_all(conn):
    cur = conn.execute("SELECT id, doc_name, page_num, text FROM pages")
    return cur.fetchall()

# =========================
# PDF
# =========================
def extract_pdf_pages(pdf_bytes: bytes):
    reader = PdfReader(pdf_bytes)
    pages = []
    for i, page in enumerate(reader.pages):
        try:
            text = page.extract_text() or ""
        except Exception:
            text = ""
        pages.append((i + 1, normalize(text)))
    return pages

def normalize(text: str) -> str:
    text = text.replace("\x00", " ")
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# =========================
# Index
# =========================
@dataclass
class IndexBundle:
    vectorizer: TfidfVectorizer
    matrix: Any
    page_ids: List[int]

def build_index(conn):
    rows = db_fetch_all(conn)
    texts = [r[3] for r in rows]
    page_ids = [r[0] for r in rows]

    vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(3, 6),
        min_df=1,
        max_df=0.95
    )
    matrix = vectorizer.fit_transform(texts)
    return IndexBundle(vectorizer, matrix, page_ids)

def search(conn, bundle: IndexBundle, query: str, k: int = 5):
    qv = bundle.vectorizer.transform([query])
    sims = cosine_similarity(qv, bundle.matrix).flatten()
    idxs = sims.argsort()[::-1][:k]

    results = []
    for i in idxs:
        if sims[i] <= 0:
            continue
        pid = bundle.page_ids[i]
        row = conn.execute(
            "SELECT doc_name, page_num, text FROM pages WHERE id=?",
            (pid,)
        ).fetchone()
        results.append({
            "score": float(sims[i]),
            "doc": row[0],
            "page": row[1],
            "text": row[2][:300] + "..."
        })
    return results

# =========================
# UI
# =========================
st.set_page_config(page_title="Lectureâ€“Exam Matcher", layout="wide")
st.title("ðŸ“š Lectureâ€“Exam Matcher (Demo)")

user_id = st.sidebar.text_input("User ID", "demo_user")
conn = db_connect(user_db_path(user_id))

index_path = user_index_path(user_id)
bundle = joblib.load(index_path) if os.path.exists(index_path) else None

tab1, tab2 = st.tabs(["ðŸ“¤ Upload & Index", "ðŸ” Search"])

with tab1:
    files = st.file_uploader("Upload PDFs", type="pdf", accept_multiple_files=True)
    if st.button("Save PDFs"):
        for f in files:
            pages = extract_pdf_pages(f.getvalue())
            db_insert_pages(conn, f.name, pages)
        st.success("PDFs saved")

    if st.button("Build Index"):
        bundle = build_index(conn)
        joblib.dump(bundle, index_path)
        st.success("Index built")

with tab2:
    query = st.text_input("Search keyword")
    if st.button("Search") and bundle:
        results = search(conn, bundle, query)
        for r in results:
            st.markdown(
                f"**{r['doc']} p.{r['page']}** (score {r['score']:.2f})"
            )
            st.write(r["text"])
# --- ê¸°ì¡´ ë§¤ì¹­ ê²°ê³¼ê°€ 'df_results'ë¼ëŠ” ë°ì´í„°í”„ë ˆìž„ì— ìžˆë‹¤ê³  ê°€ì •í•  ë•Œ ---

st.divider() # ì‹œê°ì  êµ¬ë¶„ì„ 
st.header("ðŸŽ¯ ìˆ˜ì—… í›„: ë³µìŠµ ë° ë‹¨ê¶Œí™” ì§€ì›")

# 1. íƒ­ìœ¼ë¡œ ê¸°ëŠ¥ ë¶„ë¥˜ (ê¹”ë”í•œ UI)
tab1, tab2, tab3 = st.tabs(["ðŸ“„ ë‹¨ê¶Œí™” ë…¸íŠ¸", "ðŸ§  ì•”ê¸° ì¹´ë“œ(Anki)", "ðŸ¤– AI í€´ì¦ˆ"])

with tab1:
    st.subheader("ì¡±ë³´ ì£¼ì„ í¬í•¨ PDF ìƒì„±")
    st.write("ê°•ì˜ë¡ì˜ ê¸°ì¶œ êµ¬ê°„ì— ì¡±ë³´ ë²ˆí˜¸ë¥¼ ìžë™ìœ¼ë¡œ ìž…ížŒ PDFë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
    if st.button("ë‹¨ê¶Œí™” PDF ë‹¤ìš´ë¡œë“œ (ì¤€ë¹„ ì¤‘)"):
        # ì—¬ê¸°ì— PyMuPDF(fitz) ë“±ì„ í™œìš©í•œ PDF íŽ¸ì§‘ ë¡œì§ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.
        st.info("í˜„ìž¬ ê°œë°œ ì¤‘ì¸ ê¸°ëŠ¥ìž…ë‹ˆë‹¤. ê¸°ì¶œ ìœ„ì¹˜ê°€ í‘œì‹œëœ ë ˆì´ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

with tab2:
    st.subheader("Anki ì¹´ë“œ ì„¸íŠ¸ ì¶”ì¶œ")
    st.write("ì˜¤ëŠ˜ ë§¤ì¹­ëœ ì¡±ë³´ ë¬¸í•­ì„ ê¸°ë°˜ìœ¼ë¡œ Anki(.csv) íŒŒì¼ì„ ë§Œë“­ë‹ˆë‹¤.")
    
    # ì˜ˆì‹œ ë°ì´í„° ìƒì„± ë¡œì§
    if not df_results.empty:
        anki_data = df_results[['lecture_keyword', 'exam_content']].rename(
            columns={'lecture_keyword': 'Front', 'exam_content': 'Back'}
        )
        csv = anki_data.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="Ankiìš© CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name='medical_anki_cards.csv',
            mime='text/csv',
        )
    else:
        st.warning("ë§¤ì¹­ëœ ë°ì´í„°ê°€ ì—†ì–´ ì¹´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.subheader("AI ì˜ˆì¸¡ ë³€í˜• ë¬¸ì œ")
    # GPT APIê°€ ì—°ê²°ë˜ì–´ ìžˆë‹¤ë©´ ë§¤ì¹­ëœ ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì „ë‹¬
    if st.button("ì˜¤ëŠ˜ì˜ í•µì‹¬ í€´ì¦ˆ ìƒì„±"):
        with st.spinner('AIê°€ ì¡±ë³´ íŒ¨í„´ì„ ë¶„ì„ ì¤‘...'):
            # ê°€ìƒì˜ ê²°ê³¼ ì˜ˆì‹œ
            st.success("ë¶„ì„ ì™„ë£Œ!")
            st.markdown("""
            **Q. ë‹¤ìŒ ì¤‘ ì˜¤ëŠ˜ ë°°ìš´ 'A ê¸°ì „'ì˜ ì¡±ë³´ ë¹ˆì¶œ ì˜¤ë‹µ ìœ í˜•ì€?**
            1. ì¦ìƒê³¼ ì•½ë¬¼ì„ ë°˜ëŒ€ë¡œ ë§¤ì¹­
            2. ë°œë³‘ ì‹œê¸°ë¥¼ 2ì£¼ì—ì„œ 4ì£¼ë¡œ ë³€ê²½
            3. ìœ ì „ í˜•ì‹ì„ ìš°ì„±ì—ì„œ ì—´ì„±ìœ¼ë¡œ ë³€ê²½
            
            *ì •ë‹µì€ **3ë²ˆ**ìž…ë‹ˆë‹¤. ìž‘ë…„ ì¡±ë³´ì—ì„œ ì´ ë¶€ë¶„ì´ í•¨ì •ìœ¼ë¡œ ë‚˜ì™”ìŠµë‹ˆë‹¤.*
            """)
