import os
import re
import sqlite3
from dataclasses import dataclass
from typing import List, Tuple

import streamlit as st
from pypdf import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import joblib

# =========================
# Config
# =========================
APP_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(APP_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

def user_dir(user_id: str) -> str:
    d = os.path.join(DATA_DIR, safe_filename(user_id))
    os.makedirs(d, exist_ok=True)
    return d

def user_db_path(user_id: str) -> str:
    return os.path.join(user_dir(user_id), "user.db")

def user_index_path(user_id: str) -> str:
    return os.path.join(user_dir(user_id), "tfidf_index.joblib")

def safe_filename(s: str) -> str:
    s = s.strip()
    s = re.sub(r"[^a-zA-Z0-9ê°€-í£._-]+", "_", s)
    return s[:64] if s else "user"

# =========================
# DB
# =========================
def db_connect(db_path: str):
    conn = sqlite3.connect(db_path)
    conn.execute("""
    CREATE TABLE IF NOT EXISTS pages (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        doc_name TEXT NOT NULL,
        page_num INTEGER NOT NULL,
        text TEXT NOT NULL
    );
    """)
    conn.execute("""
    CREATE TABLE IF NOT EXISTS meta (
        key TEXT PRIMARY KEY,
        value TEXT
    );
    """)
    conn.commit()
    return conn

def db_insert_pages(conn, doc_name: str, pages: List[Tuple[int, str]]):
    conn.executemany(
        "INSERT INTO pages(doc_name, page_num, text) VALUES (?, ?, ?)",
        [(doc_name, pnum, txt) for pnum, txt in pages if txt and txt.strip()]
    )
    conn.commit()

def db_clear(conn):
    conn.execute("DELETE FROM pages;")
    conn.execute("DELETE FROM meta;")
    conn.commit()

def db_fetch_all_pages(conn) -> List[Tuple[int, str, int, str]]:
    # returns (id, doc_name, page_num, text)
    cur = conn.execute("SELECT id, doc_name, page_num, text FROM pages ORDER BY id ASC;")
    return cur.fetchall()

# =========================
# PDF Parsing
# =========================
def extract_pdf_pages(pdf_bytes: bytes) -> List[Tuple[int, str]]:
    reader = PdfReader(pdf_bytes)
    out = []
    for i, page in enumerate(reader.pages):
        try:
            txt = page.extract_text() or ""
        except Exception:
            txt = ""
        txt = normalize_text(txt)
        out.append((i + 1, txt))
    return out

def normalize_text(t: str) -> str:
    t = t.replace("\x00", " ")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

# =========================
# Indexing & Search
# =========================
@dataclass
class IndexBundle:
    vectorizer: TfidfVectorizer
    matrix
    page_ids: List[int]  # maps row -> pages.id

def build_index(conn) -> IndexBundle:
    rows = db_fetch_all_pages(conn)
    if not rows:
        raise ValueError("No pages to index.")

    page_ids = [r[0] for r in rows]
    texts = [r[3] for r in rows]

    # í•œêµ­ì–´ëŠ” í˜•íƒœì†Œ ë¶„ì„ ì—†ì´ë„ ë°ëª¨ëŠ” ì¶©ë¶„íˆ ê°€ëŠ¥í•˜ê²Œ char-ngramsë¡œ ì¡ìŠµë‹ˆë‹¤.
    vectorizer = TfidfVectorizer(
        analyzer="char_wb",
        ngram_range=(3, 6),
        min_df=1,
        max_df=0.95
    )
    matrix = vectorizer.fit_transform(texts)
    return IndexBundle(vectorizer=vectorizer, matrix=matrix, page_ids=page_ids)

def search_index(conn, bundle: IndexBundle, query: str, top_k: int = 8):
    query = normalize_text(query)
    if not query:
        return []

    qv = bundle.vectorizer.transform([query])
    sims = cosine_similarity(qv, bundle.matrix).flatten()
    ranked = sims.argsort()[::-1][:top_k]

    # fetch rows for ranked page_ids
    results = []
    for idx in ranked:
        score = float(sims[idx])
        if score <= 0:
            continue
        page_id = bundle.page_ids[idx]
        cur = conn.execute("SELECT doc_name, page_num, text FROM pages WHERE id=?;", (page_id,))
        row = cur.fetchone()
        if row:
            doc_name, page_num, text = row
            results.append({
                "score": score,
                "doc_name": doc_name,
                "page_num": page_num,
                "snippet": make_snippet(text, query),
                "full_text": text
            })
    return results

def make_snippet(text: str, query: str, width: int = 240) -> str:
    if not text:
        return ""
    q = query.strip()
    pos = text.find(q)
    if pos == -1:
        # fallback: first chunk
        return (text[:width] + "â€¦") if len(text) > width else text
    start = max(0, pos - width // 3)
    end = min(len(text), pos + width)
    snippet = text[start:end]
    if start > 0:
        snippet = "â€¦" + snippet
    if end < len(text):
        snippet = snippet + "â€¦"
    return snippet

# =========================
# Simple â€œNote Draftâ€ (Post-class)
# =========================
def extract_key_sentences(text: str, max_sentences: int = 8) -> List[str]:
    # ì•„ì£¼ ë‹¨ìˆœí•œ ë¬¸ì¥ ë¶„ë¦¬ + ê¸¸ì´ ê¸°ë°˜ í•„í„° (ë°ëª¨ìš©)
    sents = re.split(r"(?<=[.!?ã€‚]|ë‹¤\.)\s+", text.strip())
    sents = [s.strip() for s in sents if len(s.strip()) >= 20]
    return sents[:max_sentences]

def draft_one_page_note(lecture_text: str, matched_pages: List[dict]) -> str:
    out = []
    out.append("## ì˜¤ëŠ˜ ê°•ì˜ í•µì‹¬ ìš”ì•½(ì´ˆì•ˆ)")
    for s in extract_key_sentences(lecture_text, 6):
        out.append(f"- {s}")

    out.append("\n## ì¡±ë³´/ê¸°ì¶œ ì—°ê²°(ì´ˆì•ˆ)")
    if not matched_pages:
        out.append("- (ë§¤ì¹­ëœ í˜ì´ì§€ ì—†ìŒ) í‚¤ì›Œë“œë¥¼ ë” êµ¬ì²´í™”í•´ë³´ì„¸ìš”.")
    else:
        for r in matched_pages[:6]:
            out.append(f"- [{r['doc_name']} p.{r['page_num']}] (ìœ ì‚¬ë„ {r['score']:.3f}) {r['snippet']}")

    out.append("\n## ë‹¨ê¶Œí™” ì •ë¦¬ í…œí”Œë¦¿(ì±„ì›Œë„£ê¸°)")
    out.append("- ì •ì˜/ê°œë…:")
    out.append("- ì™œ ì¤‘ìš”í•œê°€(ì‹œí—˜ í¬ì¸íŠ¸):")
    out.append("- ìì£¼ ë‚˜ì˜¤ëŠ” ë¬¸ì œ íŒ¨í„´:")
    out.append("- ì‹¤ìˆ˜ í¬ì¸íŠ¸/í•¨ì •:")
    out.append("- 1ì¤„ ì•”ê¸°ë¬¸ì¥:")
    return "\n".join(out)

# =========================
# UI
# =========================
st.set_page_config(page_title="ì¡±ë³´-ê°•ì˜ ë§¤ì¹­ ë°ëª¨", layout="wide")

st.title("ğŸ“š ì¡±ë³´/ê°•ì˜ ë§¤ì¹­ ë°ëª¨ (ì„¸ë¯¸ í´ë¡œì¦ˆë“œ + ë²Œí¬ ì—…ë¡œë“œ + ë§¤ì¹­)")

with st.sidebar:
    st.subheader("ğŸ” ë°ëª¨ ë¡œê·¸ì¸")
    user_id = st.text_input("ë‹‰ë„¤ì„(ì‚¬ìš©ì ID)", value=st.session_state.get("user_id", "taeyop"))
    user_id = safe_filename(user_id)
    st.session_state["user_id"] = user_id

    st.divider()
    st.caption("ì‚¬ìš©ìë³„ë¡œ DB/ì¸ë±ìŠ¤ê°€ ë¶„ë¦¬ë©ë‹ˆë‹¤.")
    st.write(f"í˜„ì¬ ì‚¬ìš©ì: **{user_id}**")

# Init DB
conn = db_connect(user_db_path(user_id))

# Load index if exists
bundle = None
index_path = user_index_path(user_id)
if os.path.exists(index_path):
    try:
        bundle = joblib.load(index_path)
    except Exception:
        bundle = None

tab1, tab2, tab3, tab4 = st.tabs(["1) ì—…ë¡œë“œ/ì¸ë±ì‹±", "2) Pre-class(ì˜ˆìŠµ ì¶”ì²œ)", "3) In-class(ì¦‰ì‹œ ë§¤ì¹­)", "4) Post-class(ë‹¨ê¶Œí™” ì´ˆì•ˆ)"])

# -------------------------
# 1) Upload / Index
# -------------------------
with tab1:
    st.subheader("1) ë²Œí¬ ì—…ë¡œë“œ + ìë™ ì¸ë±ì‹±")
    colA, colB = st.columns([2, 1], gap="large")

    with colA:
        files = st.file_uploader(
            "PDF ì—¬ëŸ¬ ê°œë¥¼ í•œ ë²ˆì— ì˜¬ë ¤ì£¼ì„¸ìš” (ì¡±ë³´/ê°•ì˜ë¡ ë“±)",
            type=["pdf"],
            accept_multiple_files=True
        )

        if st.button("ğŸ“¥ ì—…ë¡œë“œ ë°˜ì˜(í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ DB ì €ì¥)"):
            if not files:
                st.warning("ì—…ë¡œë“œí•  PDFë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                total_pages = 0
                for f in files:
                    pages = extract_pdf_pages(f.getvalue())
                    db_insert_pages(conn, f.name, pages)
                    total_pages += len(pages)
                st.success(f"ì™„ë£Œ: {len(files)}ê°œ PDF, ì´ {total_pages}í˜ì´ì§€ë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    with colB:
        st.write("**í˜„ì¬ ë°ì´í„° ìƒíƒœ**")
        rows = db_fetch_all_pages(conn)
        st.metric("ì €ì¥ëœ í˜ì´ì§€ ìˆ˜", len(rows))

        if st.button("ğŸ§  ì¸ë±ìŠ¤ ë¹Œë“œ/ê°±ì‹  (ê²€ìƒ‰ ì¤€ë¹„)"):
            rows = db_fetch_all_pages(conn)
            if not rows:
                st.warning("DBì— í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            else:
                bundle = build_index(conn)
                joblib.dump(bundle, index_path)
                st.success("ì¸ë±ìŠ¤ë¥¼ ìƒì„±/ê°±ì‹ í–ˆìŠµë‹ˆë‹¤. ì´ì œ ê²€ìƒ‰/ë§¤ì¹­ì´ ë©ë‹ˆë‹¤.")

        if st.button("ğŸ—‘ï¸ ì´ ì‚¬ìš©ì ë°ì´í„° ì´ˆê¸°í™”(ë°ëª¨ìš©)"):
            db_clear(conn)
            if os.path.exists(index_path):
                os.remove(index_path)
            st.success("DB/ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

    st.divider()
    st.subheader("ğŸ” ë¹ ë¥¸ ê²€ìƒ‰(ì „ì²´)")
    query = st.text_input("ê²€ìƒ‰ì–´(í‚¤ì›Œë“œ/ë¬¸ì¥)", placeholder="ì˜ˆ: ì‚°í™”í™˜ì›, Simpson rule, í˜ˆì•• ì¡°ì ˆ, ...")
    topk = st.slider("ìƒìœ„ ê²°ê³¼ ìˆ˜", 3, 15, 8)

    if st.button("ê²€ìƒ‰ ì‹¤í–‰"):
        if bundle is None:
            st.error("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì¸ë±ìŠ¤ ë¹Œë“œ/ê°±ì‹ 'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        else:
            results = search_index(conn, bundle, query, top_k=topk)
            if not results:
                st.info("ê²°ê³¼ê°€ ì—†ì–´ìš”. í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
            else:
                for r in results:
                    with st.expander(f"({r['score']:.3f}) {r['doc_name']} / p.{r['page_num']}"):
                        st.write(r["snippet"])
                        st.caption("ì›ë¬¸(ì¼ë¶€)")
                        st.text(r["full_text"][:1200] + ("â€¦" if len(r["full_text"]) > 1200 else ""))

# -------------------------
# 2) Pre-class
# -------------------------
with tab2:
    st.subheader("2) Pre-class: ì˜¤ëŠ˜ ê°•ì˜ ì˜ˆìŠµ(ê¸°ì¶œ ë¹„ì¤‘ ë†’ì€ í˜ì´ì§€ ì¶”ì²œ)")

    st.write("ê°•ì˜ ì£¼ì œ/í‚¤ì›Œë“œë¥¼ ë„£ìœ¼ë©´, ì—…ë¡œë“œí•œ ì¡±ë³´/ê°•ì˜ë¡ì—ì„œ **ê°€ì¥ ê´€ë ¨ ë†’ì€ í˜ì´ì§€**ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.")
    lecture_topic = st.text_area("ì˜¤ëŠ˜ ë“¤ì„ ê°•ì˜ í‚¤ì›Œë“œ(ì—¬ëŸ¬ ì¤„ ê°€ëŠ¥)", height=120, placeholder="ì˜ˆ: ì‹¬ì¥ ì „ê¸°ìƒë¦¬, í™œë™ì „ìœ„, refractory period ...")

    if st.button("ğŸ¯ ì˜ˆìŠµ ì¶”ì²œ ìƒì„±"):
        if bundle is None:
            st.error("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì—…ë¡œë“œ/ì¸ë±ì‹± íƒ­ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.")
        else:
            results = search_index(conn, bundle, lecture_topic, top_k=10)
            if not results:
                st.info("ì¶”ì²œì´ ì•ˆ ë‚˜ì™”ì–´ìš”. í‚¤ì›Œë“œë¥¼ ë” êµ¬ì²´í™”í•´ë³´ì„¸ìš”.")
            else:
                st.success("ì˜ˆìŠµ ì¶”ì²œ í˜ì´ì§€ TOP 10")
                for r in results:
                    st.markdown(f"- **{r['doc_name']} p.{r['page_num']}** (ìœ ì‚¬ë„ {r['score']:.3f}) â€” {r['snippet']}")

# -------------------------
# 3) In-class
# -------------------------
with tab3:
    st.subheader('3) In-class: "êµìˆ˜ë‹˜ì´ ì¤‘ìš”í•˜ë‹¤" ìˆœê°„ ì¦‰ì‹œ ë§¤ì¹­(ë°ëª¨)')
    st.write("ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ê¹Œì§€ëŠ” ë°ëª¨ 2ì°¨ì—ì„œ ë¶™ì´ê³ , 1ì°¨ ë°ëª¨ì—ì„œëŠ” **í•µì‹¬ í‚¤ì›Œë“œ ì…ë ¥ â†’ ì¦‰ì‹œ ìš°ì¸¡ íŒ¨ë„ ë§¤ì¹­**ìœ¼ë¡œ ì²´ê°í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.")

    colL, colR = st.columns([1, 1], gap="large")
    with colL:
        live_keyword = st.text_input('êµìˆ˜ë‹˜ ë©˜íŠ¸/í‚¤ì›Œë“œ(ì˜ˆ: "ì´ê±° ì‹œí—˜ì— ë‚˜ì˜´", "Starling curve")')
        if st.button("âš¡ ì¦‰ì‹œ ë§¤ì¹­"):
            if bundle is None:
                st.error("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.session_state["live_results"] = search_index(conn, bundle, live_keyword, top_k=8)

    with colR:
        st.write("### ğŸ“Œ ê´€ë ¨ ê¸°ì¶œ/ì¡±ë³´(ìš°ì¸¡ íŒ¨ë„)")
        results = st.session_state.get("live_results", [])
        if not results:
            st.caption("ì™¼ìª½ì—ì„œ í‚¤ì›Œë“œë¥¼ ë„£ê³  'ì¦‰ì‹œ ë§¤ì¹­'ì„ ëˆŒëŸ¬ë³´ì„¸ìš”.")
        else:
            for r in results:
                st.markdown(f"**({r['score']:.3f}) {r['doc_name']} / p.{r['page_num']}**")
                st.write(r["snippet"])
                st.divider()

# -------------------------
# 4) Post-class
# -------------------------
with tab4:
    st.subheader("4) Post-class: ê°•ì˜ë¡ + ì¡±ë³´ë¥¼ í•©ì¹œ 'ë‚˜ë§Œì˜ ë‹¨ê¶Œí™” ë…¸íŠ¸' ì´ˆì•ˆ")
    st.write("ê°•ì˜ ë©”ëª¨(ë˜ëŠ” ê°•ì˜ë¡ í…ìŠ¤íŠ¸)ë¥¼ ë¶™ì—¬ë„£ìœ¼ë©´, ê´€ë ¨ í˜ì´ì§€ë¥¼ ëŒì–´ì™€ì„œ ë‹¨ê¶Œí™” í…œí”Œë¦¿ìœ¼ë¡œ ì´ˆì•ˆì„ ë½‘ìŠµë‹ˆë‹¤. (ë°ëª¨ 1ì°¨ëŠ” ë¹„LLM)")

    lecture_note = st.text_area("ì˜¤ëŠ˜ ê°•ì˜ ë©”ëª¨/ê°•ì˜ë¡ í…ìŠ¤íŠ¸", height=180, placeholder="ìˆ˜ì—… ì§í›„ ë©”ëª¨ë¥¼ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê¸°")
    match_hint = st.text_input("ì¶”ê°€ ë§¤ì¹­ íŒíŠ¸(ì„ íƒ)", placeholder="ì˜ˆ: 'ì‹œí—˜', 'ê¸°ì¶œ', 'ì •ì˜', íŠ¹ì • ê°œë…ëª…â€¦")

    if st.button("ğŸ§¾ ë‹¨ê¶Œí™” ë…¸íŠ¸ ì´ˆì•ˆ ìƒì„±"):
        if bundle is None:
            st.error("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            q = lecture_note + "\n" + match_hint
            matched = search_index(conn, bundle, q, top_k=10)
            draft = draft_one_page_note(lecture_note, matched)
            st.text_area("ìƒì„±ëœ ì´ˆì•ˆ", value=draft, height=360)

st.caption("ë°ëª¨ 1ì°¨: ë¡œì»¬ì—ì„œ ëŒì•„ê°€ëŠ” ê²€ìƒ‰/ë§¤ì¹­ ì²´ê°ìš© MVP (ì‚¬ìš©ìë³„ DB ë¶„ë¦¬ + ë²Œí¬ ì—…ë¡œë“œ + í˜ì´ì§€ ë§¤ì¹­).")
